#!/usr/bin/env python
"""Command line script for driving the XENON1T event builder

The event builder is responsible for turning the DAQ output (a stream of dititizer pulses)
into a zipped BSON with events ready for processing.

This code will need to make a connection to the XENON1T runs database, as the event builder
uses the runs database database to locate what data needs to be processed,
get appropriate settings for each run, and write status information when done with a run.

The data generated by the event builder will be placed in /data/xenon/raw/run_name.
"""
import argparse
import logging
import os
import time
import datetime
import shutil

from six.moves import input

from pax import parallel, units
from pax.configuration import combine_configs, fix_sections_from_mongo
from pax.MongoDB_ClientMaker import PersistentRunsDBConnection

# Name of the field in the run doc which holds the trigger status
STATUS_FIELD_NAME = 'trigger.status'
TRIGGERED_DATA_HOSTNAME = 'xe1t-datamanager'

# Global used for runs-db interaction (needed in several functions)
rundb = None


def main():
    """Start running the event builder
    """
    global rundb
    args = get_command_line_arguments()
    log = setup_logging(loglevel=args.log.upper())

    # Check for strange argument combinations
    not_searching_full_run = ((args.stop_after_sec and (0 < args.stop_after_sec < float('inf'))) or
                              (args.start_after_sec and (0 < args.start_after_sec < float('inf'))))
    if not_searching_full_run and not args.secret_mode:
        print("You are about to trigger on a piece of a run, which means you are probably testing "
              "something out. However, you have NOT enabled secret mode and will be writing to official runs db!\n"
              "Are you sure??")
        if input().lower() not in ('y', 'yes'):
            print("\nFine, Exiting event builder...\n")
            exit()
    if args.secret_mode and not (args.run or args.run_name):
        raise NotImplementedError("Secret mode doesn't support wait-for-runs-mode (too lazy right now)")
    if args.secret_mode and args.deleter:
        raise NotImplementedError("Deleter mode doesn't support secret mode (would probably be a bad idea)")

    # Connect to MongoDB
    rundb = PersistentRunsDBConnection({k: getattr(args, k) for k in ('user', 'password', 'host', 'port')})

    # Build the query used to search for runs
    if args.deleter:
        status_in, status_pending, status_done = 'deleteme', 'deleting', 'deleted'
    else:
        status_in, status_pending, status_done = 'waiting_to_be_processed', 'staging', 'processed'

    if args.run_name:
        run_query = {'name': args.run_name}
    elif args.run is not None:
        run_query = {'number': args.run}
    else:
        run_query = {STATUS_FIELD_NAME: status_in,
                     'data.type': 'untriggered'}
    run_query['detector'] = args.detector
    log.info("Starting run search with query %s" % run_query)

    # Define the suffix for the produced folder
    if args.detector == "muon_veto":
        detector_suffix = "_MV"
    else:
        detector_suffix = ""

    failed_last_time = False

    while 1:
        rundb.check()

        ##
        # Get a run to process
        ##
        if not args.secret_mode:
            # Get a run and immediately set the pending status
            run_doc = rundb.collection.find_one_and_update(run_query, {'$set': {STATUS_FIELD_NAME: status_pending}})

        else:
            # Just get a run, don't update the status to pending
            run_doc = rundb.collection.find_one(run_query)

        if args.deleter:
            # Insert a status doc to show we are running
            # For the deleter this doesn't contain any other information
            rundb.pipeline_status_collection.insert({'name': 'deleter',
                                                     'time': datetime.datetime.utcnow()})

        if run_doc is None:
            if args.wait <= 0:
                log.info("No runs to process: exiting.")
                break
            log.info("No runs to process yet... waiting %d seconds", args.wait)
            if not args.secret_mode and not args.deleter:
                rundb.pipeline_status_collection.insert({'name': 'eventbuilder_info',
                                                         'time': datetime.datetime.utcnow(),
                                                         'working_on_run': False})
            time.sleep(args.wait)
            continue

        if args.deleter:
            # Our job is to delete data
            from pax.plugins.io.MongoDB import connect_to_eb_dbs
            _, _, dbs, _ = connect_to_eb_dbs(
                clientmaker=rundb.clientmaker,
                run_doc=run_doc,
                detector=args.detector,
                split_collections=run_doc['reader']['ini'].get('rotating_collections', False))

            # Give some warning
            print("I'm going to delete the data for run %d (%s). "
                  "If you don't want this you have some time to hit CTRL-C..." % (run_doc['number'], run_doc['name']))
            del_timeout = 300
            for t in range(del_timeout // 10):
                print("%d seconds left...\n" % (del_timeout - 10 * t))
                time.sleep(10)

            print("Starting deletion of untriggered data")
            # Delete all collections from this run
            for db in dbs:
                for coll_name in db.collection_names():
                    if not coll_name.startswith(run_doc['name']):
                        continue
                    db.drop_collection(coll_name)

            print("Removing untriggered data entry")
            # First refresh the run doc, perhaps new data entries got added while we were waiting for deletion
            # (e.g. some transfer started somewhere)
            # TODO: It would be safer to change this to an atomic find one and update
            run_doc = rundb.collection.find_one({'_id': run_doc['_id']})
            rundb.collection.update_one({'_id': run_doc['_id']},
                                        {'$set': {'data': [d for d in run_doc['data']
                                                           if d['type'] != 'untriggered']}})

            rundb.collection.find_one_and_update({'_id': run_doc['_id']},
                                                 {'$set': {STATUS_FIELD_NAME: status_done}})

            # Continue searching for runs (rest of code is for normal eventbuilder, we never want to reach that)
            continue

        # Our job is to trigger the run

        if args.secret_mode:
            # Create a "data entry" that only lives in memory
            data_info_entry = dict(location=os.path.join('.', run_doc['name'] + detector_suffix))

        else:
            # Add a new data entry to the data list in the run doc
            data_info_entry = {'type': 'raw',
                               'host': TRIGGERED_DATA_HOSTNAME,
                               'status': 'transferring',
                               'location': os.path.join('/data/xenon/raw',
                                                        run_doc['name'] + detector_suffix),
                               'creation_time': datetime.datetime.utcnow(),
                               'checksum': None}
            rundb.collection.update({'_id': run_doc['_id']},
                                    {'$push': {'data': data_info_entry}})

        # Build configuration for pax. Settings priority:
        #   First priority: command line args
        #   Second priority: rundoc's trigger mode and trigger_config_override (latter has higher priority)
        #   Third priority: defaults from pax configuration (pax will take care of loading these)
        # Unit conversion factors are applied to the command line args, but NOT to the rundb values!
        # (to do that, we'd have to un-convert them when we later store the settings we actually used)

        # Load run doc settings + basic settings
        # This is a
        config_dict = {}
        trigger_mode = run_doc['reader']['ini']['trigger_mode']
        if trigger_mode.startswith('triggered'):
            log.info("Picked up triggered mode %s, special settings activated" % trigger_mode)
            config_dict = {"Trigger": {
                               "left_extension": 10 * units.us,
                               "right_extension": 1010 * units.us if trigger_mode.endswith('long') else 100 * units.us,
                               "event_separation": 2 * units.ms if trigger_mode.endswith('long') else 110 * units.us,
                           },
                           "Trigger.DecideTriggers": {
                               "trigger_probability": {
                                   "0": {"2": 1},
                                   "1": {"2": 1},
                                   "2": {"2": 1}
                               }
                           }}

        config_dict = combine_configs(config_dict, {
            'DEFAULT': {'run_doc_id': run_doc['_id'],
                        'run_number': run_doc['number']},
            'pax': {'output_name': data_info_entry['location']},
            'BSON': {'fields_to_ignore': [],
                     'overwrite_output': True}})

        # Set any extra settings specified in the run doc
        mongo_conf = fix_sections_from_mongo(run_doc['reader']['ini'].get('trigger_config_override', {}))
        config_dict = combine_configs(config_dict, mongo_conf)

        # Apply command-line override settings for MongoDB
        mongo_settings_override = {}
        for argname in ('detector', 'secret_mode',
                        'host', 'port', 'user', 'password',
                        'batch_window', 'edge_safety_margin',
                        'max_query_workers', 'skip_ahead', 'stop_after_sec', 'start_after_sec'):
            argval = getattr(args, argname)
            if argval is None or argval == '':
                continue
            if argname in ('edge_safety_margin', 'batch_window'):
                argval *= units.s
            mongo_settings_override[argname] = argval
        config_dict = combine_configs(config_dict, dict(MongoDB=mongo_settings_override))

        # Acquisition monitor filename is a mongo db option...
        if config_dict['MongoDB'].get('save_acquisition_monitor_pulses', True):
            config_dict['MongoDB']['acquisition_monitor_file_path'] = os.path.join(
                data_info_entry['location'], 'acquisition_monitor_data.pickles')

        # Apply command-line override settings for trigger
        # Trigger monitor filename is a trigger option...
        trigger_settings_override = {'trigger_monitor_file_path': os.path.join(data_info_entry['location'],
                                                                               'trigger_monitor_data.zip')}

        for argname, factor in (('left_extension', units.us),
                                ('right_extension', units.us),
                                ('signal_separation', units.us),
                                ('event_separation', units.us)):
            argval = getattr(args, argname)
            if argval is not None:
                if factor is not None:
                    argval = int(factor * argval)
                trigger_settings_override[argname] = argval
        config_dict = combine_configs(config_dict, dict(Trigger=trigger_settings_override))

        # Apply command-line override settings for trigger probability
        if args.force_multiplicity_trigger is not None:
            str_level = str(args.force_multiplicity_trigger)
            config_dict = combine_configs(config_dict, {'Trigger.DecideTriggers': {'trigger_probability': {
                '0': {str_level: 1},
                '1': {str_level: 1},
                '2': {str_level: 1}}}})

        log.info("Building events for %s", run_doc['name'])
        log.info("Config override is: %s" % config_dict)
        config_kwargs = dict(config_names=['XENON1T' if args.detector == 'tpc' else 'XENON1T_MV'] + args.config,
                             config_paths=args.config_path,
                             config_dict=config_dict)
        try:
            parallel.maybe_multiprocess(args, **config_kwargs)

        except Exception as e:
            # Something went wrong and we had to stop event building.
            # Stop acquisition of the run to avoid overfilling the MongoDB buffer.
            # Hopefully the next run will work...
            log.fatal("Caught exception, aborting this run (not clearing queue).")
            log.fatal(str(type(e)) + ": " + str(e))

            if not args.secret_mode:
                mark_run_errored(run_doc)
                stop_acquisition()
                communicate_alert(e,
                                  run_number=run_doc['number'],
                                  priority=3 if failed_last_time else 0)

            failed_last_time = True

        else:
            # Things went fine, tell the runs DB
            if not args.secret_mode:
                data_info_entry['status'] = 'verifying'
                rundb.collection.update({'_id': run_doc['_id'],
                                         'data.host': data_info_entry['host']},
                                        {'$set': {'data.$': data_info_entry}})

            failed_last_time = False

        finally:
            # Whether or not a crash occurred, we should copy out the log file.
            # Otherwise the error log would be overwritten when the next run continues / the trigger restarts.
            if os.path.exists('eventbuilder.log'):
                if not os.path.exists(data_info_entry['location']):
                    os.makedirs(data_info_entry['location'])
                shutil.copyfile('eventbuilder.log',
                                data_info_entry['location'] + '/eventbuilder.log')
                # Clear the log file, so previous run starts with a clean slate
                open('eventbuilder.log', mode='w').close()
            else:
                log.fatal('No eventbuilder logfile to copy out?')

        # No need to set status == 'trigger_done' here, that's done in the trigger itself
        # (hm, maybe not very consistent...)

        # If we're trying to build a single run, don't try again to find it
        if args.run_name or args.run:
            break


def stop_acquisition():
    """Stop acquisition of the current run"""
    rundb.check()
    rundb.db['daq_control'].insert(dict(
        command='Stop',
        detector='all',
        user='trigger',
        comment='Caught exception in trigger'))


def communicate_alert(e, run_number=None, priority=3):
    """Write an exception e to the log and send an alert the DAQ database (visible on the website)"""
    log = logging.getLogger('Eventbuilder')
    log.exception(e)
    log.error("Communicating alert to run DB")

    rundb.check()
    rundb.db['log'].insert(dict(
        priority=priority,                # Low priority is less severe
        time=datetime.datetime.utcnow(),
        run=run_number,
        message=str(e),
        sender='trigger',
    ))


def mark_run_errored(run_doc):
    """Sets the current run's trigger status as 'error'"""
    rundb.check()

    query_this_run = {'name': run_doc['name'], 'detector': run_doc['detector']}
    rundb.collection.update_one(query_this_run,
                                {'$set': {STATUS_FIELD_NAME: 'error'}})

    # Find the data entry for the triggered data we were making, and set it to 'error' instead of transferring
    # We need to start by re-fetching the run doc, as the data entries will have changed since we first queried it
    run_doc = rundb.collection.find_one(query_this_run)
    for i, d in enumerate(run_doc['data']):
        if d['host'] == TRIGGERED_DATA_HOSTNAME and d['type'] == 'raw':
            run_doc['data'][i]['status'] = 'error'
    rundb.collection.update_one(query_this_run, {'$set': {'data': run_doc['data']}})


def setup_logging(loglevel='DEBUG'):
    # Set up logging to file
    logging.basicConfig(level=loglevel,
                        format='%(asctime)s %(name)s %(levelname)-8s %(message)s',
                        datefmt='%m-%d %H:%M',
                        filename='eventbuilder.log',
                        filemode='w')
    # define a Handler which writes INFO messages or higher to the sys.stderr
    console = logging.StreamHandler()
    console.setLevel(logging.DEBUG)
    # set a format which is simpler for console use
    formatter = logging.Formatter('%(asctime)s %(name)-12s: '
                                  '%(levelname)-8s %(message)s')
    # tell the handler to use this format
    console.setFormatter(formatter)
    # add the handler to the root logger
    logging.getLogger('').addHandler(console)
    return logging.getLogger('Eventbuilder')


def get_command_line_arguments():
    """Return the parsed arguments from the command line (from ArgumentParser.parse_args())
    """
    parser = argparse.ArgumentParser(description="Build XENON1T events from the DAQ.")

    # Deleter mode
    parser.add_argument('--deleter', action='store_true',
                        help='Instead of building events, look for runs that can be deleted and clear them.')

    # Multiprocessing
    mp_group = parser.add_argument_group(title='Multiprocessing')
    mp_group.add_argument('--cpus', default=1, type=int,
                          help="Number of CPUs to use. If >1, will activate multiprocessing and use 2 + cpus cores.")
    mp_group.add_argument('--remote',  action='store_true',
                          help="Multiprocess using remote workers")
    parallel.add_rabbit_command_line_args(mp_group)

    parser.add_argument('--config',
                        default=['eventbuilder'],
                        nargs='+',
                        help="Name(s) of the pax configuration(s) to use, default is eventbuilder. ")
    parser.add_argument('--config_path',
                        default=[],
                        nargs='+',
                        help="Path(s) of the configuration file(s) to use.")

    parser.add_argument('--detector', default='tpc', type=str,
                        help="Detector to build events for")
    parser.add_argument('--log', default='INFO',
                        help="Logging level to use, e.g. DEBUG")
    parser.add_argument('--secret_mode', action='store_true',
                        help="Never write anything to the runs db. Use only for testing!")
    parser.add_argument('--wait', default=10, type=int,
                        help="Wait time before searching for new runs again if none were found (sec). "
                             "If 0, instead shuts down if no new runs are found.")

    connection_group = parser.add_argument_group(title='MongoDB connection settings')
    # TODO: Unfortunate we have to specify defaults here and again in configuration...
    connection_group.add_argument('--host',type=str, default='gw',
                                  help='MongoDB hostname')
    connection_group.add_argument('--port', type=int, default=27017,
                                  help='Port on host where MongoDB runs')
    connection_group.add_argument('--user', type=str, default='eb',
                                  help='User to connect to MongoDB')
    connection_group.add_argument('--password', type=str, default=os.environ.get('MONGO_PASSWORD'),
                                  help='Password to connect to MongoDB. '
                                       'If not provided, will try to use MONGO_PASSWORD from env')

    single_run_group = parser.add_mutually_exclusive_group()
    single_run_group.add_argument('--run_name', type=str,
                                  help="Instead of building all waiting_to_be_processed runs, "
                                       "look for this specific run name and build it.")
    single_run_group.add_argument('--run', type=int,
                                  help="Instead of building all waiting_to_be_processed runs, "
                                       "look for this specific run number and build it.")

    trigger_group = parser.add_argument_group(title='Trigger settings',
                                              description='Override default settings for the trigger.')
    trigger_group.add_argument('--force_multiplicity_trigger', type=int,
                               help='Forces the trigger into a more basic mode: always trigger on a signal of more than'
                                    'force_multiplicity_trigger pulses, and never on smaller signals.')
    trigger_group.add_argument('--signal_separation', type=float,
                               help='If no pulses start for this length of time (us),'
                                    'a signal ends / a new signal can start.')
    trigger_group.add_argument('--event_separation', type=float,
                               help='If there are no triggers for this length of time (us),'
                                    'a new event can start.')
    trigger_group.add_argument('--left_extension', type=float,
                               help="Minimum range before each trigger to save (us). Is abs()ed before it is used.")
    trigger_group.add_argument('--right_extension', type=float,
                               help='Minimum time after trigger to save (us).')

    mongoreader_group = parser.add_argument_group(title='Trigger driving / Mongo reading settings')
    mongoreader_group.add_argument('--batch_window', type=float,
                                   help='Length of time range (in sec, float) to get pulse data in each query.')
    mongoreader_group.add_argument('--edge_safety_margin', type=int,
                                   help="When triggering live, stay at least this many seconds "
                                        "away from the insertion edge.")
    mongoreader_group.add_argument('--max_query_workers', type=int,
                                   help="Maximum number of parallel pulse time range queries "
                                        "to fire off in the trigger. For the deleter, it is the "
                                        "number of parallel delete queries.")
    mongoreader_group.add_argument('--skip_ahead', type=int, default=0,
                                   help="After triggering a batch of data, skip ahead this number of batches "
                                        "without triggering. "
                                        "Use only for dealing quickly with runs that have overflown to disk!")
    mongoreader_group.add_argument('--stop_after_sec', type=float,
                                   help="Stop the trigger after this many seconds of data have been read. "
                                        "Will round to query batch. Use only for testing!")
    mongoreader_group.add_argument('--start_after_sec', type=float,
                                   help="Start the trigger after this many seconds into the run. "
                                        "Use only for testing!")

    args = parser.parse_args()
    return args

if __name__ == "__main__":
    main()
