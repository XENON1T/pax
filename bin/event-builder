#!/usr/bin/env python
"""Command line script for driving the XENON1T event builder

The event builder is responsible for turning the DAQ output (a stream of dititizer pulses)
into a zipped BSON with events ready for processing.

This code will need to make a connection to the XENON1T runs database, as the event builder
uses the runs database database to locate what data needs to be processed,
get appropriate settings for each run, and write status information when done with a run.

The data generated by the event builder will be placed in /data/xenon1t/run_name.
"""
import argparse
import logging
import os
import time
import datetime
import socket
import shutil

from six.moves import input

from pax import parallel, units
from pax.configuration import combine_configs, fix_sections_from_mongo
from pax.MongoDB_ClientMaker import ClientMaker

# Name of the field in the run doc which holds the trigger status
STATUS_FIELD_NAME = 'trigger.status'
TRIGGERED_DATA_HOSTNAME = 'xe1t-datamanager'

# global used for runs-db interaction (needed in several functions)
clientmaker_config = None
run_client = None
run_db = None
runs = None
pipeline_status_collection = None


def get_hostname():
    return socket.gethostname().split('.')[0]


def connect_to_mongo():
    global run_client, run_db, runs, pipeline_status_collection
    clientmaker = ClientMaker(config=clientmaker_config)
    run_client = clientmaker.get_client('run', autoreconnect=True)
    run_db = run_client['run']
    runs = run_db['runs_new']
    pipeline_status_collection = run_db['pipeline_status']
    return run_client, run_db, runs, pipeline_status_collection


def require_live_runs_db():
    """Checks that the runs db connection we currently have is alive. If not, we try to re-acquire it forever."""
    log = logging.getLogger('Eventbuilder')
    while True:
        try:
            run_client.admin.command('ping')
            return

        except Exception as e:
            log.fatal("Exception pinging runs db: %s: %s" % (type(e), str(e)))

            try:
                connect_to_mongo()
            except Exception as e:
                log.fatal("Could not re-acquire runs db connection: %s %s. Trying again in ten seconds." % (
                    type(e), str(e)))
                time.sleep(10)


def main():
    global clientmaker_config
    """Start running the event builder
    """
    args = get_command_line_arguments()
    log = setup_logging(loglevel=args.log.upper())

    # Check for strange argument combinations
    not_searching_full_run = ((args.stop_after_sec and (0 < args.stop_after_sec < float('inf'))) or
                              (args.start_after_sec and (0 < args.start_after_sec < float('inf'))))
    if not_searching_full_run and not args.secret_mode:
        print("You are about to trigger on a piece of a run, which means you are probably testing "
              "something out. However, you have NOT enabled secret mode and will be writing to official runs db!\n"
              "Are you sure??")
        if input().lower() not in ('y', 'yes'):
            print("\nFine, Exiting event builder...\n")
            exit()
    if args.secret_mode and not (args.run or args.run_name):
        raise RuntimeError("Secret mode doesn't support wait-for-runs-mode (too lazy right now)")

    # Connect to MongoDB
    clientmaker_config = {k: eval('args.%s' % k, dict(args=args))
                          for k in ('user', 'password', 'host', 'port')}
    connect_to_mongo()

    # Build the query used to search for runs
    if args.run_name:
        run_query = {'name': args.run_name}
    elif args.run is not None:
        run_query = {'number': args.run}
    else:
        run_query = {STATUS_FIELD_NAME: 'waiting_to_be_processed',
                     'data.type': 'untriggered'}
    run_query['detector'] = args.detector
    log.info("Starting run search with query %s" % run_query)

    # Define the suffix for the produced folder
    if args.detector == "muon_veto":
        detector_suffix = "_MV"
    else:
        detector_suffix = ""

    failed_last_time = False

    while 1:
        require_live_runs_db()

        if not args.secret_mode:
            run_doc = runs.find_one_and_update(run_query, {'$set': {STATUS_FIELD_NAME: 'staging'}})

            if run_doc is not None:
                data_info_entry = {'type': 'raw',
                                   'host': TRIGGERED_DATA_HOSTNAME,
                                   'status': 'transferring',
                                   'location': os.path.join('/data/xenon/raw',
                                                            run_doc['name']+detector_suffix),
                                   'creation_time': datetime.datetime.utcnow(),
                                   'checksum': None}
                runs.update({'_id': run_doc['_id']},
                            {'$push': {'data': data_info_entry}})
        else:
            run_doc = runs.find_one(run_query)
            if run_doc is None:
                raise ValueError("Mongodb returned None instead of run document!")
            data_info_entry = dict(location=os.path.join('.',
                                                         run_doc['name']+detector_suffix))

        if run_doc is None:
            if args.wait <= 0:
                log.info("No runs to process: exiting.")
                break
            log.info("No runs to process yet... waiting %d seconds", args.wait)
            if not args.secret_mode:
                pipeline_status_collection.insert({'name': 'eventbuilder_info',
                                                   'time': datetime.datetime.utcnow(),
                                                   'working_on_run': False})
            time.sleep(args.wait)
            continue

        # Build configuration for pax. Settings priority:
        #   First priority: command line args
        #   Second priority: rundoc's trigger mode and trigger_config_override (latter has higher priority)
        #   Third priority: defaults from pax configuration (pax will take care of loading these)
        # Unit conversion factors are applied to the command line args, but NOT to the rundb values!
        # (to do that, we'd have to un-convert them when we later store the settings we actually used)

        # Load run doc settings + basic settings
        # This is a
        config_dict = {}
        trigger_mode = run_doc['reader']['ini']['trigger_mode']
        if trigger_mode.startswith('triggered'):
            log.info("Picked up triggered mode %s, special settings activated" % trigger_mode)
            config_dict = {"Trigger": {
                               "left_extension": 10 * units.us,
                               "right_extension": 1010 * units.us if trigger_mode.endswith('long') else 100 * units.us,
                               "event_separation": 2 * units.ms if trigger_mode.endswith('long') else 110 * units.us,
                           },
                           "Trigger.DecideTriggers": {
                               "trigger_probability": {
                                   "0": {"2": 1},
                                   "1": {"2": 1},
                                   "2": {"2": 1}
                               }
                           }}

        config_dict = combine_configs(config_dict, {
            'DEFAULT': {'run_doc_id': run_doc['_id'],
                        'run_number': run_doc['number']},
            'pax': {'output_name': data_info_entry['location']},
            'BSON': {'fields_to_ignore': [],
                     'overwrite_output': True}})

        # Set any extra settings specified in the run doc
        mongo_conf = fix_sections_from_mongo(run_doc['reader']['ini'].get('trigger_config_override', {}))
        config_dict = combine_configs(config_dict, mongo_conf)

        # Apply command-line override settings for MongoDB
        mongo_settings_override = {}
        for argname in ('detector', 'secret_mode',
                        'host', 'port', 'user', 'password',
                        'batch_window', 'delete_data', 'edge_safety_margin',
                        'max_query_workers', 'skip_ahead', 'stop_after_sec', 'start_after_sec'):
            argval = getattr(args, argname)
            if argval is None or argval == '':
                continue
            if argname in ('edge_safety_margin', 'batch_window'):
                argval *= units.s
            mongo_settings_override[argname] = argval
        config_dict = combine_configs(config_dict, dict(MongoDB=mongo_settings_override))

        # Acquisition monitor filename is a mongo db option...
        if config_dict['MongoDB'].get('save_acquisition_monitor_pulses', True):
            config_dict['MongoDB']['acquisition_monitor_file_path'] = os.path.join(
                data_info_entry['location'], 'acquisition_monitor_data.pickles')

        # Apply command-line override settings for trigger
        # Trigger monitor filename is a trigger option...
        trigger_settings_override = {'trigger_monitor_file_path': os.path.join(data_info_entry['location'],
                                                                               'trigger_monitor_data.zip')}

        for argname, factor in (('left_extension', units.us),
                                ('right_extension', units.us),
                                ('signal_separation', units.us),
                                ('event_separation', units.us)):
            argval = getattr(args, argname)
            if argval is not None:
                if factor is not None:
                    argval = int(factor * argval)
                trigger_settings_override[argname] = argval
        config_dict = combine_configs(config_dict, dict(Trigger=trigger_settings_override))

        # Apply command-line override settings for trigger probability
        if args.force_multiplicity_trigger is not None:
            str_level = str(args.force_multiplicity_trigger)
            config_dict = combine_configs(config_dict, {'Trigger.DecideTriggers': {'trigger_probability': {
                '0': {str_level: 1},
                '1': {str_level: 1},
                '2': {str_level: 1}}}})

        log.info("Building events for %s", run_doc['name'])
        log.info("Config override is: %s" % config_dict)
        config_kwargs = dict(config_names=['XENON1T' if args.detector == 'tpc' else 'XENON1T_MV'] + args.config,
                             config_paths=args.config_path,
                             config_dict=config_dict)
        try:
            parallel.maybe_multiprocess(args, **config_kwargs)

        except Exception as e:
            # Something went wrong and we had to stop event building.
            # Stop acquisition of the run to avoid overfilling the MongoDB buffer.
            # Hopefully the next run will work...
            log.fatal("Caught exception, aborting this run (not clearing queue).")
            log.fatal(str(type(e)) + ": " + str(e))

            if not args.secret_mode:
                mark_run_errored(run_doc)
                stop_acquisition()
                communicate_alert(e,
                                  run_number=run_doc['number'],
                                  priority=3 if failed_last_time else 0)

            failed_last_time = True

        else:
            # Things went fine, tell the runs DB
            if not args.secret_mode:
                data_info_entry['status'] = 'verifying'
                runs.update({'_id': run_doc['_id'],
                             'data.host': data_info_entry['host']},
                            {'$set': {'data.$': data_info_entry}})

            failed_last_time = False

        finally:
            # Whether or not a crash occurred, we should copy out the log file.
            # Otherwise the error log would be overwritten when the next run continues / the trigger restarts.
            if os.path.exists('eventbuilder.log'):
                if not os.path.exists(data_info_entry['location']):
                    os.makedirs(data_info_entry['location'])
                shutil.copyfile('eventbuilder.log',
                                data_info_entry['location'] + '/eventbuilder.log')
                # Clear the log file, so previous run starts with a clean slate
                open('eventbuilder.log', mode='w').close()
            else:
                log.fatal('No eventbuilder logfile to copy out?')

        # If we're trying to build a single run, don't try again to find it
        if args.run_name or args.run:
            break


def stop_acquisition():
    """Stop acquisition of the current run"""
    require_live_runs_db()
    daq_control = run_db['daq_control']
    insert_doc = {}
    insert_doc['command'] = 'Stop'
    insert_doc['detector'] = 'all'
    insert_doc['user'] = 'trigger'
    insert_doc['comment'] = 'Caught exception in trigger'
    daq_control.insert(insert_doc)


def communicate_alert(e, run_number=None, priority=3):
    """Write an exception e to the log and send an alert the DAQ database (visible on the website)"""
    log = logging.getLogger('Eventbuilder')
    log.exception(e)
    log.error("Communicating alert to run DB")

    require_live_runs_db()
    alerts = run_db['log']
    alert_doc = {"priority": priority,   # Low priority is less severe
                 "time": datetime.datetime.now(),
                 "run": run_number,
                 "message": str(e),
                 "sender": "trigger"}
    alerts.insert(alert_doc)


def mark_run_errored(run_doc):
    """Sets the current run's trigger status as 'error'"""
    require_live_runs_db()

    query_this_run = {'name': run_doc['name'], 'detector': run_doc['detector']}
    runs.update_one(query_this_run,
                    {'$set': {STATUS_FIELD_NAME: 'error'}})

    # Find the data entry for the triggered data we were making, and set it to 'error' instead of transferring
    # We need to start by re-fetching the run doc, as the data entries will have changed since we first queried it
    run_doc = runs.find_one(query_this_run)
    for i, d in enumerate(run_doc['data']):
        if d['host'] == TRIGGERED_DATA_HOSTNAME and d['type'] == 'raw':
            run_doc['data'][i]['status'] = 'error'
    runs.update_one(query_this_run, {'$set': {'data': run_doc['data']}})


def setup_logging(loglevel='DEBUG'):
    # Set up logging to file
    logging.basicConfig(level=loglevel,
                        format='%(asctime)s %(name)s %(levelname)-8s %(message)s',
                        datefmt='%m-%d %H:%M',
                        filename='eventbuilder.log',
                        filemode='w')
    # define a Handler which writes INFO messages or higher to the sys.stderr
    console = logging.StreamHandler()
    console.setLevel(logging.DEBUG)
    # set a format which is simpler for console use
    formatter = logging.Formatter('%(asctime)s %(name)-12s: '
                                  '%(levelname)-8s %(message)s')
    # tell the handler to use this format
    console.setFormatter(formatter)
    # add the handler to the root logger
    logging.getLogger('').addHandler(console)
    return logging.getLogger('Eventbuilder')


def get_command_line_arguments():
    """Return the parsed arguments from the command line (from ArgumentParser.parse_args())
    """
    parser = argparse.ArgumentParser(description="Build XENON1T events from the DAQ.")

    # Multiprocessing
    mp_group = parser.add_argument_group(title='Multiprocessing')
    mp_group.add_argument('--cpus', default=1, type=int,
                          help="Number of CPUs to use. If >1, will activate multiprocessing and use 2 + cpus cores.")
    mp_group.add_argument('--remote',  action='store_true',
                          help="Multiprocess using remote workers")
    parallel.add_rabbit_command_line_args(mp_group)

    parser.add_argument('--config',
                        default=['eventbuilder'],
                        nargs='+',
                        help="Name(s) of the pax configuration(s) to use, default is eventbuilder. ")
    parser.add_argument('--config_path',
                        default=[],
                        nargs='+',
                        help="Path(s) of the configuration file(s) to use.")

    parser.add_argument('--detector', default='tpc', type=str,
                        help="Detector to build events for")
    parser.add_argument('--log', default='INFO',
                        help="Logging level to use, e.g. DEBUG")
    parser.add_argument('--secret_mode', action='store_true',
                        help="Never write anything to the runs db. Use only for testing!")
    parser.add_argument('--wait', default=10, type=int,
                        help="Wait time before searching for new runs again if none were found (sec). "
                             "If 0, instead shuts down if no new runs are found.")

    connection_group = parser.add_argument_group(title='MongoDB connection settings')
    # TODO: Unfortunate we have to specify defaults here and again in configuration...
    connection_group.add_argument('--host',type=str, default='gw',
                                  help='MongoDB hostname')
    connection_group.add_argument('--port', type=int, default=27017,
                                  help='Port on host where MongoDB runs')
    connection_group.add_argument('--user', type=str, default='eb',
                                  help='User to connect to MongoDB')
    connection_group.add_argument('--password', type=str, default=os.environ.get('MONGO_PASSWORD'),
                                  help='Password to connect to MongoDB. '
                                       'If not provided, will try to use MONGO_PASSWORD from env')

    single_run_group = parser.add_mutually_exclusive_group()
    single_run_group.add_argument('--run_name', type=str,
                                  help="Instead of building all waiting_to_be_processed runs, "
                                       "look for this specific run name and build it.")
    single_run_group.add_argument('--run', type=int,
                                  help="Instead of building all waiting_to_be_processed runs, "
                                       "look for this specific run number and build it.")

    trigger_group = parser.add_argument_group(title='Trigger settings',
                                              description='Override default settings for the trigger.')
    trigger_group.add_argument('--force_multiplicity_trigger', type=int,
                               help='Forces the trigger into a more basic mode: always trigger on a signal of more than'
                                    'force_multiplicity_trigger pulses, and never on smaller signals.')
    trigger_group.add_argument('--signal_separation', type=float,
                               help='If no pulses start for this length of time (us),'
                                    'a signal ends / a new signal can start.')
    trigger_group.add_argument('--event_separation', type=float,
                               help='If there are no triggers for this length of time (us),'
                                    'a new event can start.')
    trigger_group.add_argument('--left_extension', type=float,
                               help="Minimum range before each trigger to save (us). Is abs()ed before it is used.")
    trigger_group.add_argument('--right_extension', type=float,
                               help='Minimum time after trigger to save (us).')

    mongoreader_group = parser.add_argument_group(title='Trigger driving / Mongo reading settings')
    mongoreader_group.add_argument('--batch_window', type=float,
                                   help='Length of time range (in sec, float) to get pulse data in each query.')
    mongoreader_group.add_argument('--delete_data', action='store_true',
                                   help="Delete data from the untriggered collection while triggering on it. "
                                        "Essential for running the trigger live for an extended time!")
    mongoreader_group.add_argument('--edge_safety_margin', type=int,
                                   help="When triggering live, stay at least this many seconds "
                                        "away from the insertion edge.")
    mongoreader_group.add_argument('--max_query_workers', type=int,
                                   help="Maximum number of parallel pulse time range queries "
                                        "to fire off in the trigger. If delete_data, this is also the maximum "
                                        "number of parallel delete queries.")
    mongoreader_group.add_argument('--skip_ahead', type=int, default=0,
                                   help="After triggering a batch of data, skip ahead this number of batches "
                                        "without triggering. "
                                        "Use only for dealing quickly with runs that have overflown to disk!")
    mongoreader_group.add_argument('--stop_after_sec', type=float,
                                   help="Stop the trigger after this many seconds of data have been read. "
                                        "Will round to query batch. Use only for testing!")
    mongoreader_group.add_argument('--start_after_sec', type=float,
                                   help="Start the trigger after this many seconds into the run. "
                                        "Use only for testing!")

    args = parser.parse_args()
    return args

if __name__ == "__main__":
    main()
