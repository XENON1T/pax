##
# Base, TPC-independent configuration for pax
# Does NOT contain a complete working set of settings - you must load a TPC settings file!
##

# All values are run with eval()
#
# Unit convention: all values carrying physical units must be specified carrying units.
#   GOOD: resistor = 50 * Ohm
#   BAD:  resistor = 50         This is not just bad style, but will actually fail to do what you want,
#                               as the number representing 1 Ohm resistance internally is not 1.
# If you specify a count of something (number of samples, digitizer counts...),
# please indicate what is being counted using a comment, unless it is really obvious.
# GOOD: min_s2_width = 50 #Samples
#  BAD: min_s2_width = 50
#   OK: num_pmts = 242


[pax]
# Settings for pax core go here

# Plugin groups: plugins from these groups will be loaded (in order)
plugin_group_names = ['input', 'pre_dsp', 'dsp', 'compute_properties', 'pre_analysis', 'pre_output', 'output']

# Plugins group lists
pre_dsp = []
dsp = []
compute_properties = []
pre_analysis = []
pre_output = ['DeleteLowLevelInfo.DeleteLowLevelInfo']
encoder_plugin = 'ROOTClass.EncodeROOTClass'
output = 'ROOTClass.WriteROOTClass'

# Logging control
logging_level = 'INFO'

# Alternate search paths for plugins (none by default, see core for where it searches for plugins):
plugin_paths = []

# Prints a report on the time taken by each plugin at end of processing
print_timing_report = True



# Global settings, passed to every plugin
[DEFAULT]
tpc_name = "some random TPC which didn't specify its name in the config file"
pmt_0_is_fake = False
run_number = 0   # TODO: get from run database
overwrite_output = 'yes'    # Set to 'confirm' if you prefer to get a warning when overwriting a file
                            # Do NOT set this if you are multiprocessing!

# Unique pax id during remote multiprocessing. Not used otherwise.
pax_id = 'not_set'

# PMT ranges - to be filled by TPC config
channels_top = []
channels_bottom = []

# CAEN V1724 used by XENON100 and XENON1T
sample_duration = int(10 * ns)      # int is necessary for data structure
digitizer_voltage_range = 2.25 * V
nominal_gain = 2.0* 10**6 #e-/pe    # Will not be used unless build_nominally_gain_corrected_waveforms=True below
digitizer_bits = 14
pmt_circuit_load_resistor = 50 * Ohm
external_amplification = 10

# Each channel's baseline will be determined dynamically
# This is merely the reference point
digitizer_reference_baseline = 16000

# width & rist_time bound, in ns for s1&s2 classification
s1_width_threshold = 300
s1_risetime_threshold = 70

photon_area_distribution = 'XENON1T_spe_distributions.csv'
spe_cutoff_sigma = -5

# Window within which hit maxima can count towards the tight coincidence level.
# Must be an odd number of digitizer samples (otherwise will be slightly wider than you specify)
tight_coincidence_window_left = 50 * ns
tight_coincidence_window_right = 50 * ns
# number of coincidence channels inside a short time window. SR0+SR1 SI uses 3-fold coincidence.
tight_coincidence_threshold = 0

[Queues]
timeout_after_sec = 300


[SumWaveform.SumWaveform]
# If true, the 'tpc_raw', 'veto_raw' sum waveforms will be constructed WITHOUT subtracting the baseline correction
# Useful to see effect of baseline correction
# Note the raw sum waveform is for cosmetic (plotting) purposes only,
# it isn't (and shouldn't) be used for anything else
subtract_reference_baseline_only_for_raw_waveform = False


[BasicProperties.SumWaveformProperties]

[BuildInteractions.BasicInteractionProperties]
# Statistic to use for the S1 pattern goodness of fit: same options as for PosRecTopPatternFit
s1_pattern_statistic = 'likelihood_poisson'

# Set this to true to include corrections for S1 and S2 saturation & zombie PMTs
# If false, the saturation corrections will still be computed, but it will not be taken into account in the final
# total correction.
include_saturation_correction = False

##
# Trigger default settings (maybe these should be in XENON1T.ini... but it's nice to try them on XENON100 data too)
# Settings specified in the runs database will override these, command line arguments to event-builder override both.
##
[Trigger]
# List of trigger plugins to run, in the specified order
trigger_plugins = [
    'SortData',
    'HandleEdgePulses',
    'FindSignals',
    'DeadTimeTally',
    'ClassifySignals',
    'DecideTriggers',
    # Trigger on muon veto triggers: disabled for now until we can find signal grouping edge case this seems to cause
    # 'AnyPulseFromDetectorTrigger',
    'HandleEdgeSignals',
    'GroupTriggers',
    'SaveSignals',
    ]

# Filename for storing additional data (options below)
trigger_data_filename = 'trigger_data.hdf5'

# If no pulses start for this length of time, a signal ends / a new signal can start (NOT new event of course!)
signal_separation = 0.3 * us

# Include this much time left and right of a trigger in an event
# 'include' means: include any pulses that START within the time range  (TODO: what is logic on boundaries?)
# If you want to increase the event size, remember to update event_separation below too!
left_extension = 100 * us
right_extension = 100 * us

# If no TRIGGERS (says nothing about times or signals) occur within this length of time, a new event can start.
# Closer triggers will be merged into one event.
# This must be larger than left_extension + right_extension (or equal, if you trust I never make off by one errors...)
# to prevent data from being claimed twice by events
event_separation = 250 * us

# Interval for saving the dark rate and dead time info
dark_rate_save_interval = 1 * s

[Trigger.FindSignals]
# Every ... saver intervals, save the full 2-pmt coincidence matrix rather than just the dark rate
dark_monitor_full_save_every = 60

# Size of the trigger's internal signal buffer.
# The memory use is about 100 bytes per signal, so 1M signals is roughly 100 MB RAM.
# If the buffer is full, it will be extended by this same amount (so this is a really really unimportant setting)
numba_signal_buffer_size = int(1e6)


[Trigger.ClassifySignals]
# Maximum possible pulse start time RMS for S1
s1_max_rms = 100 * ns

# Minimum number of pulses for an S2
s2_min_pulses = 10

[Trigger.DecideTriggers]
# Trigger probability / "prescale" settings: chance to triggger on a signal as function of the number of pulses in it.
# This is a dictionary like this:
#       {signal_type: {2: chance if 2 pulses or more, 5: chance if 5 pulses or more, 42: chance if 42 pulses or more},
#        signal_type: ...}
#   signal_type is 0 for unknown, 1 for s1, 2 for s2.
# If there are less pulses than the lowest entry, no trigger will occur.
# Note 'unknown' always means you have less than s2_min_pulses pulses,
# while s2 always means you have s2_min_pulses or more.
# Note all dictionary keys must be strings, unlike in regular python: this is a json limitation.
#
# Be careful if you set a nonzero trigger probability for the 'unknown' peaks,
# these are very easily made by exponential noise.
trigger_probability = {'0': {'50': 1},
                       '1': {'50': 1},
                       '2': {'50': 1}}


[Trigger.AnyPulseFromDetectorTrigger]
typecodes_for_detector = {'muon_veto_trigger': 106}


[Trigger.GroupTriggers]
# If an event is larger than this, truncate it. This will trigger a warning message.
max_event_length = 10 * ms


[Trigger.SaveSignals]
# Controls if/how signals are saved. Options:
#   None/False:   Do not save signals outside events
#   full:         Save all signals outside events with more pulses than outside_signals_save_threshold (see below)
#   2d_histogram: Save a 2d histogram every batch. Keys and bins are hardcoded, see SaveSignals.
save_signals = '2d_histogram'

# If true, save only signals which are not also part of events
only_save_signals_outside_events = False

# If save_signals = 'full', save only signals with at least this many pulses
signals_save_threshold = 10


##
# Plotting settings
##
[Plotting]
waveforms_to_plot = (
        {'internal_name': 'tpc',      'plot_label': 'TPC (hits only)',
                    'drawstyle': 'steps', 'color':'black'},
        {'internal_name': 'tpc_raw',  'plot_label': 'TPC (raw)',
                    'drawstyle': 'steps', 'color':'black', 'alpha': 0.3},
    )
output_name = 'plots'  # Output directory for plots. If SCREEN, will show plot in interactive GUI display.
plot_format = 'png'    # Can be pdf, png, or pickle. No effect if output_dir is set.
plot_every = 1         # Skip plot_every-1 waveforms after plotting one
size_multiplier = 4    # Increase/decrease to get bigger/smaller plots

log_scale_entire_event = False
log_scale_s2 = False
log_scale_s1 = False

# List of annotation filters, possibilities are :'largest', 'largest_s1s', 'largest_s2s', 'all_s1s', 'all_s2s',
# or a combination of these. Note: must be put in a list like ['largest','all_s1',...]
# Main_s1 and main_s2 are always annotated if present, set to None for no filter at all.
annotation_filter = None
# For filters with 'largest' in their name, num_of_sections gives the number of sections the event will be cut up in
# of which only the largest will be annotated. (right now sorted by height, not area)
num_of_sections = 50    # Change this to a lower number if it's still too black

[Plotting.PlotEventSummary]
plot_largest_peaks = True      # On the top row, show zoom-ins of the largest S1, S2, and their hitpatterns

[Plotting.PlotChannelWaveforms3D]
log_scale = False

[Plotting.PeakViewer]
# Which peak should we show first? Can be 'largest', 'first', 'main_s1', 'main_s2'
# If the selection is impossible (e.g. you asked for main_s1 but there are no s1s in the event)
# the largest peak will be shown first.
starting_peak = 'largest'

# Select specific peaks to plot per event like this:
# starting_peak_per_event = {'event_number': left boundary of peak, ...}

#ROOTPlot
[ROOTPlot]
waveforms_to_plot = (
        {'internal_name': 'tpc',      'plot_label': 'TPC (hits only)'},
        {'internal_name': 'tpc_raw',  'plot_label': 'TPC (raw)'}
    )
output_name = 'plots'


[XED.ReadXED]
input_name = "XENON100_120402_2000_000000.xed"



[OnlineMonitor.OnlineMonitor]
address = "xedaq00"
database = "online"
collection = "monitor"
waveformcollection = "waveforms"


[ROOTClass]
tree_name = 'tree'

# Fields to leave out of the output.
# Be very careful when adding fields to this: that means the cpp class will lose a field!
# Instead you should let a plugin clean out the unwanted fields for you
# For example, DeleteLowLevelInfo
fields_to_ignore = ['sum_waveforms',
                    'channel_waveforms',   # Doesn't exist, is sneakily added in XerawdpImitation mode
                    'raw_data']

# A list of extra fields to add to the ROOT classes
# Dictionary with key = class name (e.g. Event or Peak)
# Value = list of 3-tuples: (name, type, code)
# Code will be evaluated, with
#   python_object: instance of the pax object (e.g. Event or Peak)
#   root_object: instance of the root object (e.g. Event or Peak)
#   field: root_object.your_field_name
#   self: the WriteRootClass plugin
extra_fields = {'Event': [('s1s', 'std::vector <Int_t>',  "field.clear()\n[field.push_back(self._get_index(s1)) for s1 in python_object.s1s()]"),
                          ('s2s', 'std::vector <Int_t>',  "field.clear()\n[field.push_back(self._get_index(s2)) for s2 in python_object.s2s()]")],
               }

# Exclude compilation of the classes from the plugin timing report
# (not the tqdm progress bar)
exclude_compilation_from_timer = True

# Mapping of python/numpy types to C++/ROOT types
type_mapping = {'float':   'Float_t',
                'float64': 'Double_t',
                'float32': 'Float_t',
                'int':     'Int_t',
                'int16':   'Short_t',
                'int32':   'Int_t',
                'int64':   'Long64_t',
                'bool':    'Bool_t',
                'bool_':   'Bool_t',
                'long':    'Long64_t',
                'str':     'TString'}

# Force the types of certain fields to be different from the default type mappings
force_types = {'start_time': 'Long64_t',
               'stop_time': 'Long64_t'}

# Name and type of structured array fields. This should really be in datastructure... but wouldn't know where
structured_array_fields = {'hits': 'Hit',
                           'all_hits': 'Hit',
                           'trigger_signals': 'TriggerSignal'}


[Table.TableWriter]
output_format = 'hdf5'      # hdf5, csv, numpy, html, json, root

append_data = False
overwrite_data = True
string_data_length = 32

# Convert data to numpy records every n events
# These take less memory, but conversion takes some time
buffer_size = 50

# Write to file every time a chunk is converted to records
# If false, or output format does not support it, will write all at end
write_in_chunks = True

# Fields to leave out of the output.
# If you dump to e.g. json, you may want to ignore area_per_channel and does_channel_contribute
# You must ignore at least one of 'all_hits' (event field) or 'hits' (peak field)
# 'sum waveforms' must always be ignored
fields_to_ignore = ['sum_waveforms',
                    'channel_waveforms',
                    'all_hits',
                    'raw_data',
                   ]

[Zip]
events_per_file = 1000


[BSON]
# By default, BSON-type formats are used for raw data
fields_to_ignore = ['all_hits',
                    'sum_waveforms',
                    'channel_waveforms',]
compresslevel = 4


[MessagePack]
fields_to_ignore = ['all_hits',
                    'sum_waveforms',
                    'channel_waveforms',]
compresslevel = 4


[XED.WriteXED]
compresslevel = 4

# At the moment our hacked Xerawdp XML only supports one file...
events_per_file = float('inf')

# If the following is enabled, we'll write only 'skip2' control words rather than proper skip control words
# this appears to be the case in the XENON100 root files (at least 120402_2000_0000)
skip2_bug = False


##
# Simulator default settings
##

[WaveformSimulator]
truth_file_name =                     'fax_truth'
event_repetitions =                   1                  # Simulate each event in the instruction file this many times (1 means: simulate just once, no repetitions)
magically_avoid_s1_excluded_pmts =    False              # Photons magically avoid PMTs excluded in S1 peakfinding in the Xerawdp Imitation
                                                         # Regular pax peakfinding doesn't exclude any pmts

# If you have a relative light yield map for your TPC, set this to True!
# The simulator uses the map to adjust the number of photons produced, which means the number of photons is
# already reduced under a dead PMT.
magically_avoid_dead_pmts =           False

# Photoelectron pulse model
pe_pulse_model =                      'exponential'      # 'exponential' or 'custom'.
                                                         # If exponential, must specify pmt_rise_time and pmt_fall_time.
                                                         # If custom, must specify pmt_pulse_ts and pmt_pulse_ys.
pulse_width_cutoff =                  5                  # Assume PMT pulse is 0 after this many rise/fall times. Does not impact performance greatly.
pmt_pulse_time_rounding =             1 * ns             # Round PMT pulse start time to this resolution, so we can exploit caching.


# Simulator performance settings
use_simplified_simulator_from =       5000 #photons      # Use faster, though slightly less accurate method for peaks with more than this number of photons
                                                         # Only works if you activate cheap_zle, otherwise photons aren't clustered into bunches first
event_padding =                       5 * us             # Padding in the event before the first and after the last photon.
                                                         # if you use the cheap_zle, bad things happen if this is smaller than the zle padding
gauss_noise_sigma        =            0 #pe/bin          # Sigma of Gaussian noise to apply to waveform. Set to 0 if you want only real noise.
gauss_noise_sigmas =                  None               # list of baseline fluctuations on each PMT channel
real_noise_file =                     None               # Must be a numpy.savez_compressed file containing 1 numpy array (row per channel) of noise data
                                                         # Set to None or False if you don't want to use real noise
real_noise_sample_mode =              'incoherent'       # 'coherent' or 'incoherent'. Former gives greater variation for limited noise file, latter ensures coherence across channels


# S1
maximum_recombination_time =          50 * ns            # Prevents crazy recombination times from tail of hyperbolic distribution
s1_detection_efficiency   =           1                  # % photons detected. TODO: replace with proper 3d light yield map
singlet_lifetime_liquid   =           3.1 * ns           # Nest 2014 p2
triplet_lifetime_liquid   =           24 * ns            # Nest 2014 p2
s1_ER_recombination_fraction =        0.9
#s1_ER_recombination_fraction =        0.6               # Only used for primary/secondary split, we don't do yield calculations here!
                                                         # Nest 2011 p4 for E = about 500 V/cm and LET 10 MeV cm^2 /g (which acc to Chepel&Araujo is for 30 keV ER (higher E, less rec.)
s1_ER_primary_singlet_fraction =      1/(1+1/0.17)       # Nest 2014 p2, converted from s/t ratio to s fraction. 0.17 +-0.05
s1_ER_secondary_singlet_fraction =    1/(1+1/0.8)        # Nest 2014 p2, assuming gamma-induced ER. 0.8 +- 0.2
s1_NR_singlet_fraction =              1/(1+1/7.8)        # Nest 2014 page 2. 7.8 +- 1.5
s1_ER_alpha_singlet_fraction =        1/(1+1/2.8)        # Nest 2014 page 2. 2.3 +- 0.51

# S2
electron_trapping_time    =           140*ns              # Nest 2014, but was obtained through fitting data
gas_drift_velocity_slope =            0.54 * mm / us / Td # Fit to Brooks et al 1982 in the 5 Td - 40 Td range
lxe_dielectric_constant =             1.874               # Wikipedia (which cites some chemistry handbook), unitless
singlet_lifetime_gas      =           5.88*ns             # Nest 2014. +- 5.5 (!!)
triplet_lifetime_gas      =           119.5               # +- 5.7 ns. xenon:xenon1t:kmiller:sr1_se_waveform_shape
                                                          # 115 ns: Fit to Xenon100 single-e S2s xenon:xenon100:analysis:single_e_waveform_model
                                                          # 100.1*ns +- 7.9: Nest 2014
singlet_fraction_gas      =           0.035               # +- 0.017. xenon:xenon1t:aalbers:single_electron_waveform_shape
                                                          # 0: Fit to Xenon100 single-e S2s xenon:xenon100:analysis:single_e_waveform_model

# Light distribution
s1_light_yield_map =                  'placeholder_map.json'
s2_light_yield_map =                  'placeholder_map.json'
s1_patterns_file =                    None
s2_patterns_file =                    None

each_pmt_afterpulse_types = {}
pmt_afterpulse_types = {}

p_double_pe_emision = 0     # Probablility of double-pe emission by the cathode


[TopPatternFit.PosRecTopPatternFit]
# If False, will only add goodness_of_fit to other algorithm's positions
skip_reconstruction = False

# List of algorithms to use, in order of preference, for posrec seed
# ... or the string 'best', in which case the algorithm whose position has the best goodness of fit
# is used as the seed
# seed_algorithms = ['PosRecNeuralNet', 'PosRecRobustWeightedMean', 'PosRecWeightedSum', 'PosRecMaxPMT']
seed_algorithms = 'best'

# If true, will treat saturated PMTs as if they do not exist
# Note that's very different from assuming they see nothing!
ignore_saturated_PMTs = True

# Minimizer to use: 'powell' or 'grid'.
# grid is a lot faster if the grid size is small.
minimizer = 'grid'
grid_size = 2 * cm   # Size of grid (diameter in both dimensions)

# Goodness of fit statistic to use 'chi2', 'chi2gamma' or 'likelihood_poisson'
statistic = 'likelihood_poisson'

# Which confidence levels to compute
# The 1 and 2 sigma levels.
# More precisely the right tail probability levels 0.317, 0.05
# for a chi square distribution with 3 degrees of freedom.
confidence_levels = [3.529, 7.814]

# Make a plot for each reconstructed position, including optional contours
plot_position = False



[TopPatternFit.PosRecTopPatternFunctionFit]
# If False, will only add goodness_of_fit to other algorithm's positions
skip_reconstruction = False

# List of algorithms to use, in order of preference, for posrec seed
# ... or the string 'best', in which case the algorithm whose position has the best goodness of fit
# is used as the seed
# seed_algorithms = ['PosRecNeuralNet', 'PosRecRobustWeightedMean', 'PosRecWeightedSum', 'PosRecMaxPMT']
seed_algorithms = 'best'

# If true, will treat saturated PMTs as if they do not exist
# Note that's very different from assuming they see nothing!
ignore_saturated_PMTs = True

# Minimizer to use: 'powell' or 'grid'.
# grid is a lot faster if the grid size is small.
minimizer = 'grid'
grid_size = 2 * cm   # Size of grid (diameter in both dimensions)

# Goodness of fit statistic to use 'chi2', 'chi2gamma' or 'likelihood_poisson'
statistic = 'likelihood_poisson'

# Which confidence levels to compute
# The 1 and 2 sigma levels.
# More precisely the right tail probability levels 0.317, 0.05
# for a chi square distribution with 3 degrees of freedom.
confidence_levels = [3.529, 7.814]

# Make a plot for each reconstructed position, including optional contours
plot_position = False



[DesaturatePulses.DesaturatePulses]
# Samples to the left of a saturated region to use for scaling the waveform of other channels.
# If there are multiple saturated regions in a pulse, we'll only use the samples before the first to make the
# reference region.
reference_region_samples = 100
# Sets the minimal number of reference samples above 1PE we need to have enough infomration to desaturate
reference_region_samples_treshold = 10
# For exclusion of 15 largest after pulsing pmt in building waveform model
large_after_pulsing_channels = [ 2, 21, 23, 28, 31, 48, 61, 66, 71, 78, 85, 87, 117, 119, 142, 157, 203, 205, 217]
# large_after_pulsing_channels_sr0 = [ 2, 21, 28, 31, 61, 62, 66, 78, 79, 85, 102, 119, 183, 205, 217]
# Number of samples to use for calculating moving average
convolution_length = 100
